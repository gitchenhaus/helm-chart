---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-state-metrics
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
spec:
  privileged: false
  volumes:
    - 'secret'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-prometheus-node-exporter
  namespace: monitor
  labels:     
    app: prometheus-node-exporter
    heritage: Helm
    release: release-name
    chart: prometheus-node-exporter-1.12.0
    jobLabel: node-exporter
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
    - 'hostPath'
  hostNetwork: true
  hostIPC: false
  hostPID: true
  hostPorts:
    - min: 0
      max: 65535
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/templates/alertmanager/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-promethe-alertmanager
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/templates/prometheus-operator/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-promethe-operator
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/templates/prometheus/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-promethe-prometheus
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
  name: release-name-kube-state-metrics
  namespace: monitor
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-node-exporter
  namespace: monitor
  labels:
    app: prometheus-node-exporter
    chart: prometheus-node-exporter-1.12.0
    release: "release-name"
    heritage: "Helm"
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/templates/alertmanager/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/templates/prometheus-operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kube-promethe-operator
  namespace: monitor
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/templates/prometheus/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kube-promethe-prometheus
  namespace: monitor
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/templates/alertmanager/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
data:
  alertmanager.yaml: "Z2xvYmFsOgogIG9wc2dlbmllX2FwaV9rZXk6ICIiCiAgcmVzb2x2ZV90aW1lb3V0OiAxbQpyZWNlaXZlcnM6Ci0gbmFtZTogZGVmYXVsdC1yZWNlaXZlcgotIG5hbWU6IGdlbmVyYWwtc2xhY2sKICBzbGFja19jb25maWdzOgogIC0gYXBpX3VybDogaHR0cHM6Ly9ob29rcy5zbGFjay5jb20vc2VydmljZXMvVENMQzJGOFI0L0IwMTcwRERVTUE1LzFFa2ZQMEZXVWNwbUhORlFzVXpiaUFTSAogICAgY2hhbm5lbDogbnVsbAogICAgaWNvbl91cmw6IGh0dHBzOi8vYXZhdGFyczMuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3UvMzM4MDQ2MgogICAgc2VuZF9yZXNvbHZlZDogdHJ1ZQogICAgdGl0bGVfbGluazogaHR0cDovL2Rldi5lbnYuc2IucmV4YmV0LmNvbTozMDgwMAogICAgdXNlcm5hbWU6IGFsZXJ0ZXIKcm91dGU6CiAgZ3JvdXBfYnk6CiAgLSBqb2IKICBncm91cF9pbnRlcnZhbDogMXMKICBncm91cF93YWl0OiA1cwogIHJlY2VpdmVyOiBnZW5lcmFsLXNsYWNrCiAgcmVwZWF0X2ludGVydmFsOiAxbQogIHJvdXRlczoKICAtIGdyb3VwX2J5OgogICAgLSBJbnN0YW5jZQogICAgZ3JvdXBfaW50ZXJ2YWw6IDFzCiAgICBncm91cF93YWl0OiAxbQogICAgbWF0Y2hfcmU6CiAgICAgIHNldmVyaXR5OiB3YXJuaW5nfGNyaXRpY2FsCiAgICByZWNlaXZlcjogIm51bGwiCiAgICByZXBlYXRfaW50ZXJ2YWw6IDFtCiAgICByb3V0ZXM6CiAgICAtIG1hdGNoOgogICAgICAgIHNlcnZpY2U6IHNjcmFwZQogICAgICByZWNlaXZlcjogZ2VuZXJhbC1zbGFjaw=="
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/psp-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
  name: psp-release-name-kube-state-metrics
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-kube-state-metrics
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/psp-clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp-release-name-prometheus-node-exporter
  labels:     
    app: prometheus-node-exporter
    heritage: Helm
    release: release-name
    chart: prometheus-node-exporter-1.12.0
    jobLabel: node-exporter
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-prometheus-node-exporter
---
# Source: kube-prometheus-stack/templates/prometheus-operator/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-kube-promethe-operator
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
- apiGroups:
  - monitoring.coreos.com
  resources:
  - alertmanagers
  - alertmanagers/finalizers
  - alertmanagerconfigs
  - prometheuses
  - prometheuses/finalizers
  - thanosrulers
  - thanosrulers/finalizers
  - servicemonitors
  - podmonitors
  - probes
  - prometheusrules
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - delete
- apiGroups:
  - ""
  resources:
  - services
  - services/finalizers
  - endpoints
  verbs:
  - get
  - create
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
---
# Source: kube-prometheus-stack/templates/prometheus-operator/psp-clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-kube-promethe-operator-psp
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-kube-promethe-operator
---
# Source: kube-prometheus-stack/templates/prometheus/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-kube-promethe-prometheus
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
# This permission are not in the kube-prometheus repo
# they're grabbed from https://github.com/prometheus/prometheus/blob/master/documentation/examples/rbac-setup.yml
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - "networking.k8s.io"
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]
---
# Source: kube-prometheus-stack/templates/prometheus/psp-clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-kube-promethe-prometheus-psp
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-kube-promethe-prometheus
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: monitor
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/psp-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.9.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
  name: psp-release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-release-name-kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: release-name-kube-state-metrics
    namespace: monitor
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/psp-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: psp-release-name-prometheus-node-exporter
  labels:     
    app: prometheus-node-exporter
    heritage: Helm
    release: release-name
    chart: prometheus-node-exporter-1.12.0
    jobLabel: node-exporter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-release-name-prometheus-node-exporter
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-node-exporter
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus-operator/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-kube-promethe-operator
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-promethe-operator
subjects:
- kind: ServiceAccount
  name: release-name-kube-promethe-operator
  namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus-operator/psp-clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-kube-promethe-operator-psp
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-promethe-operator-psp
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-operator
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-kube-promethe-prometheus
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-promethe-prometheus
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-prometheus
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus/psp-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-kube-promethe-prometheus-psp
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-promethe-prometheus-psp
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-prometheus
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/alertmanager/psp-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-kube-promethe-alertmanager
---
# Source: kube-prometheus-stack/templates/alertmanager/psp-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-kube-promethe-alertmanager
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-alertmanager
    namespace: monitor
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: monitor
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: "kube-state-metrics-2.9.2"
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-node-exporter
  namespace: monitor
  annotations:
    prometheus.io/scrape: "true"
  labels:     
    app: prometheus-node-exporter
    heritage: Helm
    release: release-name
    chart: prometheus-node-exporter-1.12.0
    jobLabel: node-exporter
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app: prometheus-node-exporter
    release: release-name
---
# Source: kube-prometheus-stack/templates/alertmanager/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    self-monitor: "true"
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
    - name: web
      nodePort: 30093
      port: 9093
      targetPort: 9093
      protocol: TCP
  selector:
    app: alertmanager
    alertmanager: release-name-kube-promethe-alertmanager
  type: "NodePort"
---
# Source: kube-prometheus-stack/templates/exporters/core-dns/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-promethe-coredns
  labels:
    app: kube-prometheus-stack-coredns
    jobLabel: coredns
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
  selector:
    k8s-app: kube-dns
---
# Source: kube-prometheus-stack/templates/prometheus-operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-promethe-operator
  namespace: monitor
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - name: https
    port: 443
    targetPort: https
  selector:
    app: kube-prometheus-stack-operator
    release: "release-name"
  type: "ClusterIP"
---
# Source: kube-prometheus-stack/templates/prometheus/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-promethe-prometheus
  namespace: monitor
  labels:
    app: kube-prometheus-stack-prometheus
    self-monitor: "true"
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - name: web
    nodePort: 30090
    port: 9090
    targetPort: 9090
  selector:
    app: prometheus
    prometheus: release-name-kube-promethe-prometheus
  type: "NodePort"
---
# Source: kube-prometheus-stack/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-prometheus-node-exporter
  namespace: monitor
  labels:     
    app: prometheus-node-exporter
    heritage: Helm
    release: release-name
    chart: prometheus-node-exporter-1.12.0
    jobLabel: node-exporter
spec:
  selector:
    matchLabels:
      app: prometheus-node-exporter
      release: release-name
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:         
        app: prometheus-node-exporter
        heritage: Helm
        release: release-name
        chart: prometheus-node-exporter-1.12.0
        jobLabel: node-exporter
    spec:
      serviceAccountName: release-name-prometheus-node-exporter
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      containers:
        - name: node-exporter
          image: "quay.io/prometheus/node-exporter:v1.0.1"
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --web.listen-address=$(HOST_IP):9100
            - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
            - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 9100
          readinessProbe:
            httpGet:
              path: /
              port: 9100
          resources:
            {}
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: kube-prometheus-stack/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: monitor
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: "kube-state-metrics-2.9.2"
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: "release-name"
    spec:
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsUser: 65534
      containers:
      - name: kube-state-metrics
        args:

        - --collectors=certificatesigningrequests


        - --collectors=configmaps


        - --collectors=cronjobs


        - --collectors=daemonsets


        - --collectors=deployments


        - --collectors=endpoints


        - --collectors=horizontalpodautoscalers


        - --collectors=ingresses


        - --collectors=jobs


        - --collectors=limitranges


        - --collectors=mutatingwebhookconfigurations


        - --collectors=namespaces


        - --collectors=networkpolicies


        - --collectors=nodes


        - --collectors=persistentvolumeclaims


        - --collectors=persistentvolumes


        - --collectors=poddisruptionbudgets


        - --collectors=pods


        - --collectors=replicasets


        - --collectors=replicationcontrollers


        - --collectors=resourcequotas


        - --collectors=secrets


        - --collectors=services


        - --collectors=statefulsets


        - --collectors=storageclasses


        - --collectors=validatingwebhookconfigurations



        - --collectors=volumeattachments




        imagePullPolicy: IfNotPresent
        image: "quay.io/coreos/kube-state-metrics:v1.9.7"
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
# Source: kube-prometheus-stack/templates/prometheus-operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-promethe-operator
  namespace: monitor
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-prometheus-stack-operator
      release: "release-name"
  template:
    metadata:
      labels:
        app: kube-prometheus-stack-operator
        
        chart: kube-prometheus-stack-11.0.2
        release: "release-name"
        heritage: "Helm"
    spec:
      containers:
        - name: kube-prometheus-stack
          image: "quay.io/prometheus-operator/prometheus-operator:v0.43.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - --kubelet-service=kube-system/release-name-kube-promethe-kubelet
            - --logtostderr=true
            - --localhost=127.0.0.1
            - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.43.1
            # Empty if statement to catch non-semver master tags that do not need the --config-reloader-image flag
            - --config-reloader-cpu=100m
            - --config-reloader-memory=25Mi
            - --web.enable-tls=true
            - --web.cert-file=cert/cert
            - --web.key-file=cert/key
            - --web.listen-address=:8443
          ports:
            - containerPort: 8443
              name: https
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: tls-secret
              mountPath: /cert
              readOnly: true
      volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: release-name-kube-promethe-admission
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: release-name-kube-promethe-operator
---
# Source: kube-prometheus-stack/templates/alertmanager/alertmanager.yaml
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  image: quay.io/prometheus/alertmanager:v0.21.0
  version: v0.21.0
  replicas: 1
  listenLocal: false
  serviceAccountName: release-name-kube-promethe-alertmanager
  externalUrl: http://release-name-kube-promethe-alertmanager.monitor:9093
  paused: false
  logFormat: "logfmt"
  logLevel:  "info"
  retention: "120h"
  routePrefix: "/"
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  portName: web
---
# Source: kube-prometheus-stack/templates/prometheus/additionalPrometheusRules.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: kube-prometheus-stack-rule-name
      namespace: monitor
      labels:
        app: kube-prometheus-stack
        
        chart: kube-prometheus-stack-11.0.2
        release: "release-name"
        heritage: "Helm"
    spec:
      groups:
        - name: scrapeJobCheck
          rules:
          - alert: scrapeDown
            annotations:
              message: Scrape 異常 - {{ $labels.instance }} 服務異常.
            expr: |
              up{job!="coredns"}  == 0
            for: 1m
            labels:
              service: Scrape
              severity: critical
        - name: kubernetesgroup
          rules:
          - alert: KubeAPIDown
            annotations:
              message: 異常 - 體彩產品 KubeApi 出現異常.
            expr: |
              absent(up{job="apiserver"} == 1)
            for: 5m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesNodeReady
            annotations:
              description: |-
                Node {{ $labels.node }} has been unready for a long time
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes Node ready (instance {{ $labels.instance }})
            expr: |
              kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesMemoryPressure
            annotations:
              description: |-
                {{ $labels.node }} has MemoryPressure condition
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes memory pressure (instance {{ $labels.instance }})
            expr: |
              kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesDiskPressure
            annotations:
              description: |-
                {{ $labels.node }} has DiskPressure condition
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes disk pressure (instance {{ $labels.instance }})
            expr: |
              kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesOutOfDisk
            annotations:
              description: |-
                {{ $labels.node }} has OutOfDisk condition
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes out of disk (instance {{ $labels.instance }})
            expr: |
              kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesOutOfCapacity
            annotations:
              description: |-
                {{ $labels.node }} is out of capacity
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes out of capacity (instance {{ $labels.instance }})
            expr: |
              sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesContainerOomKiller
            annotations:
              description: |-
                Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes container oom killer (instance {{ $labels.instance }})
            expr: |
              (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesJobFailed
            annotations:
              description: |-
                Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes Job failed (instance {{ $labels.instance }})
            expr: |
              kube_job_status_failed > 0
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesPersistentvolumeclaimPending
            annotations:
              description: |-
                PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance
                }})
            expr: |
              kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesVolumeOutOfDiskSpace
            annotations:
              description: |-
                Volume is almost full (< 10% left)
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
            expr: |
              kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesPersistentvolumeError
            annotations:
              description: |-
                Persistent volume is in bad state
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes PersistentVolume error (instance {{ $labels.instance }})
            expr: |
              kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesStatefulsetDown
            annotations:
              description: |-
                A StatefulSet went down
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
            expr: |
              (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
            for: 1m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesHpaScalingAbility
            annotations:
              description: |-
                Pod is unable to scale
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes HPA scaling ability (instance {{ $labels.instance }})
            expr: |
              kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesHpaMetricAvailability
            annotations:
              description: |-
                HPA is not able to collect metrics
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes HPA metric availability (instance {{ $labels.instance }})
            expr: |
              kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesHpaScaleCapability
            annotations:
              description: |-
                The maximum number of desired Pods has been hit
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes HPA scale capability (instance {{ $labels.instance }})
            expr: |
              kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesPodCrashLooping
            annotations:
              description: |-
                Pod {{ $labels.pod }} is crash looping
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
            expr: |
              increase(kube_pod_container_status_restarts_total[1m]) > 3
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesReplicassetMismatch
            annotations:
              description: |-
                Deployment Replicas mismatch
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})
            expr: |
              kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesDeploymentReplicasMismatch
            annotations:
              description: |-
                Deployment Replicas mismatch
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes Deployment replicas mismatch (instance {{ $labels.instance
                }})
            expr: |
              kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesStatefulsetReplicasMismatch
            annotations:
              description: |-
                A StatefulSet does not match the expected number of replicas.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance
                }})
            expr: |
              kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesDeploymentGenerationMismatch
            annotations:
              description: |-
                A Deployment has failed but has not been rolled back.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes Deployment generation mismatch (instance {{ $labels.instance
                }})
            expr: |
              kube_deployment_status_observed_generation != kube_deployment_metadata_generation
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesStatefulsetGenerationMismatch
            annotations:
              description: |-
                A StatefulSet has failed but has not been rolled back.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance
                }})
            expr: |
              kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesStatefulsetUpdateNotRolledOut
            annotations:
              description: |-
                StatefulSet update has not been rolled out.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance
                }})
            expr: |
              max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesDaemonsetRolloutStuck
            annotations:
              description: |-
                Some Pods of DaemonSet are not scheduled or not ready
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})
            expr: |
              kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 10m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesDaemonsetMisscheduled
            annotations:
              description: |-
                Some DaemonSet Pods are running where they are not supposed to run
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})
            expr: |
              kube_daemonset_status_number_misscheduled > 0
            for: 1m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesApiServerErrors
            annotations:
              description: |-
                Kubernetes API server is experiencing high error rate
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes API server errors (instance {{ $labels.instance }})
            expr: |
              sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesApiClientErrors
            annotations:
              description: |-
                Kubernetes API client is experiencing high error rate
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes API client errors (instance {{ $labels.instance }})
            expr: |
              (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
            for: 2m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesClientCertificateExpiresNextWeek
            annotations:
              description: |-
                A client certificate used to authenticate to the apiserver is expiring next week.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes client certificate expires next week (instance {{ $labels.instance
                }})
            expr: |
              apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
          - alert: KubernetesClientCertificateExpiresSoon
            annotations:
              description: |-
                A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.
                  VALUE = {{ $value }}
                  LABELS = {{ $labels }}
              summary: Kubernetes client certificate expires soon (instance {{ $labels.instance
                }})
            expr: |
              apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
            for: 0m
            labels:
              service: Kubernetes
              severity: critical
        - name: storagegroup
          rules:
          - alert: MysqlDown
            annotations:
              message: 異常 - {{ $labels.instance }} 停止服務.
            expr: |
              mysql_up == 0
            for: 1m
            labels:
              service: MySQL
              severity: critical
          - alert: MysqlRestarted
            annotations:
              message: 異常 - {{ $labels.instance }} 服務重啟.
            expr: |
              mysql_global_status_uptime < 60
            for: 1m
            labels:
              service: MySQL
              severity: critical
          - alert: MySQL Connection>3200(80%)
            annotations:
              message: 異常 - 體彩產品伺服器出現MySQL連線數使用過高的情況大於80%
            expr: |
              mysql_global_status_max_used_connections{namespace=~"monitor"} > 3200
            for: 30m
            labels:
              service: MySQL
              severity: critical
          - alert: MySQL Connection>=2800(70%),<3200(80%)
            annotations:
              message: 警告 - 體彩產品伺服器出現MySQL連線數使用過高的情況大於70%,小於80%,請收集相關資訊,並於下一個工作日提供給體彩SRE人員
            expr: |
              mysql_global_status_max_used_connections{namespace=~"monitor"} >= 2800 and mysql_global_status_max_used_connections{namespace=~"monitor"} <= 3200
            for: 30m
            labels:
              service: MySQL
              severity: critical
          - alert: MongoDB Connection>12800(80%)
            annotations:
              message: 異常 - 體彩產品伺服器出現MongoDB連線數使用過高的情況大於80%
            expr: |
              mongodb_connections{namespace="monitor",state="current"} > 12800
            for: 30m
            labels:
              service: MongoDB
              severity: critical
          - alert: MongoDB Connection>=11200(70%),<12800(80%)
            annotations:
              message: 警告 - 體彩產品伺服器出現MongoDB連線數使用過高的情況大於70%,小於80%,請收集相關資訊,並於下一個工作日提供給體彩SRE人員
            expr: |
              mongodb_connections{namespace="monitor",state="current"} >= 11200 and mongodb_connections{namespace="monitor",state="current"} <= 12800
            for: 30m
            labels:
              service: MongoDB
              severity: critical
          - alert: KafkaTopicsReplicas
            annotations:
              message: 異常 - 體彩產品Kafka Cluster服務出現異常, Partitions Per Topic 數量不一致
            expr: |
              sum(kafka_topic_partition_in_sync_replica) by (topic) < 27
            for: 0m
            labels:
              service: Kafka
              severity: critical
          - alert: KafkaDown
            annotations:
              message: 異常 - {{ $labels.instance }} 停止服務.
            expr: kafka_brokers < kafka_brokers offset 1m
            labels:
              service: Kafka
              severity: critical
          - alert: KafkaRestart
            annotations:
              message: 異常 - {{ $labels.instance }} 服務重啟.
            expr: process_start_time_seconds < process_start_time_seconds offset 1m
            labels:
              service: Kafka
              severity: critical
          - alert: MongoDown
            annotations:
              message: 異常 - {{ $labels.instance }} 停止服務.
            expr: mongodb_up == 0
            for: 1m
            labels:
              service: MongoDB
              severity: critical
          - alert: MongoRestart
            annotations:
              message: 異常 - {{ $labels.instance }} 服務重啟.
            expr: mongodb_instance_uptime_seconds <= mongodb_instance_uptime_seconds offset
              1m
            for: 1m
            labels:
              service: MongoDB
              severity: critical
          - alert: RedisDown
            annotations:
              message: 異常 - {{ $labels.instance }} 停止服務.
            expr: redis_up == 0
            for: 0m
            labels:
              service: Redis
              severity: critical
          - alert: RedisRestart
            annotations:
              message: 異常 - {{ $labels.instance }} 服務重啟.
            expr: redis_uptime_in_seconds <= redis_uptime_in_seconds offset 1m
            for: 1m
            labels:
              service: Redis
              severity: critical
          - alert: RabbitMQDown
            annotations:
              message: 異常 - {{ $labels.instance }} 停止服務.
            expr: rabbitmq_up == 0
            for: 0m
            labels:
              service: RabbitMQ
              severity: critical
          - alert: RabbitMQRestart
            annotations:
              message: 異常 - {{ $labels.instance }} 服務重啟.
            expr: rabbitmq_erlang_uptime_seconds <= rabbitmq_erlang_uptime_seconds offset
              1m
            for: 1m
            labels:
              service: RabbitMQ
              severity: critical
        - name: basic_metrics
          rules:
          - alert: CPU Usage Over 70%
            annotations:
              description: CPU Usage of {{$labels.nodeInfo}} has been over 70% for more than
                1m.
              summary: CPU Usage Over 70 % (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: (1 - avg(irate(node_cpu_seconds_total{job=~"node-.*",mode="idle"}[5m]))
              by (nodeInfo))* 100 > 70
            for: 1m
            labels:
              environment: prod
              severity: warning
              type: cpu
          - alert: CPU Usage Over 85%
            annotations:
              description: CPU Usage of {{$labels.nodeInfo}} has been over 85% for more than
                1m.
              summary: CPU Usage Over 85 % (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: (1 - avg(irate(node_cpu_seconds_total{job=~"node-.*",mode="idle"}[5m]))
              by (nodeInfo))* 100 > 85
            for: 1m
            labels:
              environment: prod
              severity: critical
              type: cpu
          - alert: Memory Usage Over 70%
            annotations:
              description: Memory Usage of {{$labels.nodeInfo}} has been over 70% for more
                than 1m.
              summary: Memory Usage Over 70% (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: (1 - (node_memory_MemAvailable_bytes{job=~"node-.*"} / (node_memory_MemTotal_bytes{job=~"node-.*"})))*
              100 > 70
            for: 1m
            labels:
              environment: prod
              severity: warning
              type: memory
          - alert: Memory Usage Over 85%
            annotations:
              description: Memory Usage of {{$labels.nodeInfo}} has been over 85% for more
                than 1m.
              summary: Memory Usage Over 85% (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: (1 - (node_memory_MemAvailable_bytes{job=~"node-.*"} / (node_memory_MemTotal_bytes{job=~"node-.*"})))*
              100 > 85
            for: 1m
            labels:
              environment: prod
              severity: critical
              type: memory
          - alert: Disk Usage Over 70%
            annotations:
              description: Disk Usage of {{$labels.nodeInfo}} has been over 70% for more than
                1m.
              summary: Disk Usage Over 70% (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: max((node_filesystem_size_bytes{job=~"node-.*",fstype=~"ext.?|xfs"}-node_filesystem_free_bytes{job=~"node-.*",fstype=~"ext.?|xfs"})
              *100/(node_filesystem_avail_bytes {job=~"node-.*",fstype=~"ext.?|xfs"}+(node_filesystem_size_bytes{job=~"node-.*",fstype=~"ext.?|xfs"}-node_filesystem_free_bytes{job=~"node-.*",fstype=~"ext.?|xfs"})))by(nodeInfo)
              > 70
            for: 1m
            labels:
              environment: prod
              severity: warning
              type: disk
          - alert: Disk Usage Over 85%
            annotations:
              description: Disk Usage of {{$labels.nodeInfo}} has been over 85% for more than
                1m.
              summary: Disk Usage Over 85% (`{{ $labels.nodeInfo }}`/`{{$value}}%`)
            expr: max((node_filesystem_size_bytes{job=~"node-.*",fstype=~"ext.?|xfs"}-node_filesystem_free_bytes{job=~"node-.*",fstype=~"ext.?|xfs"})
              *100/(node_filesystem_avail_bytes {job=~"node-.*",fstype=~"ext.?|xfs"}+(node_filesystem_size_bytes{job=~"node-.*",fstype=~"ext.?|xfs"}-node_filesystem_free_bytes{job=~"node-.*",fstype=~"ext.?|xfs"})))by(nodeInfo)
              > 85
            for: 1m
            labels:
              environment: prod
              severity: critical
              type: disk
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/mutatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name:  release-name-kube-promethe-admission
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: monitor
        name: release-name-kube-promethe-operator
        path: /admission-prometheusrules/mutate
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: kube-prometheus-stack/templates/prometheus/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: release-name-kube-promethe-prometheus
  namespace: monitor
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  alerting:
    alertmanagers:
      - namespace: monitor
        name: release-name-kube-promethe-alertmanager
        port: web
        pathPrefix: "/"
        apiVersion: v2
  image: quay.io/prometheus/prometheus:v2.22.1
  version: v2.22.1
  externalUrl: http://release-name-kube-promethe-prometheus.monitor:9090
  paused: false
  replicas: 1
  logLevel:  info
  logFormat:  logfmt
  listenLocal: false
  enableAdminAPI: false
  resources:
    limits:
      memory: 8Gi
  retention: "10d"
  routePrefix: "/"
  serviceAccountName: release-name-kube-promethe-prometheus
  serviceMonitorSelector:
    matchLabels:
      release: "release-name"

  serviceMonitorNamespaceSelector: {}
  podMonitorSelector:
    matchLabels:
      release: "release-name"

  podMonitorNamespaceSelector: {}
  probeSelector:
    matchLabels:
      release: "release-name"

  probeNamespaceSelector: {}
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  ruleNamespaceSelector: {}
  ruleSelector:
    matchLabels:
      app: kube-prometheus-stack
      release: "release-name"

  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: nfs-client
  containers:
    - env:
      - name: JAEGER_AGENT_PORT
        value: "5755"
      name: prometheus
  portName: web
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/k8s.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-k8s.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: sum(rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
    - expr: |-
        sum by (cluster, namespace, pod, container) (
          rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])
        ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
          1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    - expr: |-
        container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
          max by(namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_working_set_bytes
    - expr: |-
        container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
          max by(namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_rss
    - expr: |-
        container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
          max by(namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_cache
    - expr: |-
        container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
          max by(namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_swap
    - expr: sum(container_memory_usage_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}) by (namespace)
      record: namespace:container_memory_usage_bytes:sum
    - expr: |-
        sum by (namespace) (
            sum by (namespace, pod) (
                max by (namespace, pod, container) (
                    kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}
                ) * on(namespace, pod) group_left() max by (namespace, pod) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
    - expr: |-
        sum by (namespace) (
            sum by (namespace, pod) (
                max by (namespace, pod, container) (
                    kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"}
                ) * on(namespace, pod) group_left() max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
              1, max by (replicaset, namespace, owner_name) (
                kube_replicaset_owner{job="kube-state-metrics"}
              )
            ),
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: deployment
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: daemonset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: statefulset
      record: namespace_workload_pod:kube_pod_owner:relabel
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/kube-apiserver-availability.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-kube-apiserver-availability.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - interval: 3m
    name: kube-apiserver-availability.rules
    rules:
    - expr: |-
        1 - (
          (
            # write too slow
            sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
            -
            sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
          ) +
          (
            # read too slow
            sum(increase(apiserver_request_duration_seconds_count{verb=~"LIST|GET"}[30d]))
            -
            (
              (
                sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
                or
                vector(0)
              )
              +
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
              +
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
            )
          ) +
          # errors
          sum(code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
        )
        /
        sum(code:apiserver_request_total:increase30d)
      labels:
        verb: all
      record: apiserver_request:availability30d
    - expr: |-
        1 - (
          sum(increase(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30d]))
          -
          (
            # too slow
            (
              sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
              or
              vector(0)
            )
            +
            sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
            +
            sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
          )
          +
          # errors
          sum(code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
        )
        /
        sum(code:apiserver_request_total:increase30d{verb="read"})
      labels:
        verb: read
      record: apiserver_request:availability30d
    - expr: |-
        1 - (
          (
            # too slow
            sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
            -
            sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
          )
          +
          # errors
          sum(code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
        )
        /
        sum(code:apiserver_request_total:increase30d{verb="write"})
      labels:
        verb: write
      record: apiserver_request:availability30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"2.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"3.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"4.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"5.."}[30d]))
      record: code_verb:apiserver_request_total:increase30d
    - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
      labels:
        verb: read
      record: code:apiserver_request_total:increase30d
    - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
      labels:
        verb: write
      record: code:apiserver_request_total:increase30d
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/kube-apiserver.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-kube-apiserver.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: kube-apiserver.rules
    rules:
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1d]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1d]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1d]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1d]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
      labels:
        verb: read
      record: apiserver_request:burnrate1d
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1h]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1h]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1h]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1h]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
      labels:
        verb: read
      record: apiserver_request:burnrate1h
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[2h]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[2h]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[2h]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[2h]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
      labels:
        verb: read
      record: apiserver_request:burnrate2h
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30m]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30m]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30m]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30m]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
      labels:
        verb: read
      record: apiserver_request:burnrate30m
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[3d]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[3d]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[3d]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[3d]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
      labels:
        verb: read
      record: apiserver_request:burnrate3d
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[5m]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[5m]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[5m]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[5m]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
      labels:
        verb: read
      record: apiserver_request:burnrate5m
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[6h]))
            -
            (
              (
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[6h]))
                or
                vector(0)
              )
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[6h]))
              +
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[6h]))
            )
          )
          +
          # errors
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
      labels:
        verb: read
      record: apiserver_request:burnrate6h
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1d]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
      labels:
        verb: write
      record: apiserver_request:burnrate1d
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1h]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
      labels:
        verb: write
      record: apiserver_request:burnrate1h
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[2h]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
      labels:
        verb: write
      record: apiserver_request:burnrate2h
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[30m]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
      labels:
        verb: write
      record: apiserver_request:burnrate30m
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[3d]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
      labels:
        verb: write
      record: apiserver_request:burnrate3d
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[5m]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
      labels:
        verb: write
      record: apiserver_request:burnrate5m
    - expr: |-
        (
          (
            # too slow
            sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
            -
            sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[6h]))
          )
          +
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
        )
        /
        sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
      labels:
        verb: write
      record: apiserver_request:burnrate6h
    - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
      labels:
        verb: read
      record: code_resource:apiserver_request_total:rate5m
    - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
      labels:
        verb: write
      record: code_resource:apiserver_request_total:rate5m
    - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m]))) > 0
      labels:
        quantile: '0.99'
        verb: read
      record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))) > 0
      labels:
        quantile: '0.99'
        verb: write
      record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - expr: |-
        sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
        /
        sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
      record: cluster:apiserver_request_duration_seconds:mean5m
    - expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
      labels:
        quantile: '0.99'
      record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
      labels:
        quantile: '0.9'
      record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
      labels:
        quantile: '0.5'
      record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/kube-prometheus-general.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-kube-prometheus-general.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: kube-prometheus-general.rules
    rules:
    - expr: count without(instance, pod, node) (up == 1)
      record: count:up1
    - expr: count without(instance, pod, node) (up == 0)
      record: count:up0
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/kube-prometheus-node-recording.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-kube-prometheus-node-recording.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY (instance)
      record: instance:node_cpu:ratio
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))
      record: cluster:node_cpu:ratio
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/kubelet.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-kubelet.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: kubelet.rules
    rules:
    - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.99'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.9'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.5'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/node-exporter.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-node-exporter.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: node-exporter.rules
    rules:
    - expr: |-
        count without (cpu) (
          count without (mode) (
            node_cpu_seconds_total{job="node-exporter"}
          )
        )
      record: instance:node_num_cpu:sum
    - expr: |-
        1 - avg without (cpu, mode) (
          rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |-
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |-
        1 - (
          node_memory_MemAvailable_bytes{job="node-exporter"}
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |-
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |-
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |-
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |-
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/node-exporter.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-node-exporter
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: node-exporter
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 5% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 3% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 5% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 3% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
        summary: Network interface is reporting many receive errors.
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
        summary: Network interface is reporting many transmit errors.
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeHighNumberConntrackEntriesUsed
      annotations:
        description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodehighnumberconntrackentriesused
        summary: Number of conntrack are getting close to the limit.
      expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
    - alert: NodeTextFileCollectorScrapeError
      annotations:
        description: Node Exporter text file collector failed to scrape.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodetextfilecollectorscrapeerror
        summary: Node Exporter text file collector failed to scrape.
      expr: node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
    - alert: NodeClockSkewDetected
      annotations:
        message: Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclockskewdetected
        summary: Clock skew detected.
      expr: |-
        (
          node_timex_offset_seconds > 0.05
        and
          deriv(node_timex_offset_seconds[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds < -0.05
        and
          deriv(node_timex_offset_seconds[5m]) <= 0
        )
      for: 10m
      labels:
        severity: warning
    - alert: NodeClockNotSynchronising
      annotations:
        message: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclocknotsynchronising
        summary: Clock not synchronising.
      expr: |-
        min_over_time(node_timex_sync_status[5m]) == 0
        and
        node_timex_maxerror_seconds >= 16
      for: 10m
      labels:
        severity: warning
    - alert: NodeRAIDDegraded
      annotations:
        description: RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddegraded
        summary: RAID Array is degraded
      expr: node_md_disks_required - ignoring (state) (node_md_disks{state="active"}) > 0
      for: 15m
      labels:
        severity: critical
    - alert: NodeRAIDDiskFailure
      annotations:
        description: At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddiskfailure
        summary: Failed device in RAID array
      expr: node_md_disks{state="fail"} > 0
      labels:
        severity: warning
---
# Source: kube-prometheus-stack/templates/prometheus/rules-1.14/node.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-kube-promethe-node.rules
  namespace: monitor
  labels:
    app: kube-prometheus-stack
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  groups:
  - name: node.rules
    rules:
    - expr: sum(min(kube_pod_info{node!=""}) by (cluster, node))
      record: ':kube_pod_info_node_count:'
    - expr: |-
        topk by(namespace, pod) (1,
          max by (node, namespace, pod) (
            label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
        ))
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |-
        count by (cluster, node) (sum by (node, cpu) (
          node_cpu_seconds_total{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        ))
      record: node:node_num_cpu:sum
    - expr: |-
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
---
# Source: kube-prometheus-stack/templates/alertmanager/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-alertmanager
  namespace: monitor
  labels:
    app: kube-prometheus-stack-alertmanager
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  selector:
    matchLabels:
      app: kube-prometheus-stack-alertmanager
      release: "release-name"
      self-monitor: "true"
  namespaceSelector:
    matchNames:
      - "monitor"
  endpoints:
  - port: web
    path: "/metrics"
---
# Source: kube-prometheus-stack/templates/exporters/core-dns/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-coredns
  namespace: monitor
  labels:
    app: kube-prometheus-stack-coredns
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  selector:
    matchLabels:
      app: kube-prometheus-stack-coredns
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: kube-prometheus-stack/templates/exporters/kube-api-server/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-apiserver
  namespace: monitor
  labels:
    app: kube-prometheus-stack-apiserver
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    port: https
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kubernetes
      insecureSkipVerify: false
  jobLabel: component
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      component: apiserver
      provider: kubernetes
---
# Source: kube-prometheus-stack/templates/exporters/kube-state-metrics/serviceMonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-kube-state-metrics
  namespace: monitor
  labels:
    app: kube-prometheus-stack-kube-state-metrics
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: app.kubernetes.io/name
  endpoints:
  - port: http
    honorLabels: true
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: "release-name"
---
# Source: kube-prometheus-stack/templates/exporters/kubelet/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-kubelet
  namespace: monitor
  labels:
    app: kube-prometheus-stack-kubelet    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  endpoints:
  - port: https-metrics
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    relabelings:
    - sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    honorLabels: true
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabelings:
    - sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/probes
    honorLabels: true
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabelings:
    - sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kubelet
---
# Source: kube-prometheus-stack/templates/exporters/node-exporter/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-node-exporter
  namespace: monitor
  labels:
    app: kube-prometheus-stack-node-exporter
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  selector:
    matchLabels:
      app: prometheus-node-exporter
      release: release-name
  endpoints:
  - port: metrics
    relabelings:
    - action: replace
      regex: ^(.*)$
      replacement: $1
      separator: ;
      sourceLabels:
      - __meta_kubernetes_pod_node_name
      targetLabel: nodeInfo
---
# Source: kube-prometheus-stack/templates/prometheus-operator/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-operator
  namespace: monitor
  labels:
    app: kube-prometheus-stack-operator
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  endpoints:
  - port: https
    scheme: https
    tlsConfig:
      serverName: release-name-kube-promethe-operator
      ca:
        secret:
          name: release-name-kube-promethe-admission
          key: ca
          optional: false
    honorLabels: true
  selector:
    matchLabels:
      app: kube-prometheus-stack-operator
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "monitor"
---
# Source: kube-prometheus-stack/templates/prometheus/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-promethe-prometheus
  namespace: monitor
  labels:
    app: kube-prometheus-stack-prometheus
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  selector:
    matchLabels:
      app: kube-prometheus-stack-prometheus
      release: "release-name"
      self-monitor: "true"
  namespaceSelector:
    matchNames:
      - "monitor"
  endpoints:
  - port: web
    path: "/metrics"
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/validatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name:  release-name-kube-promethe-admission
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: monitor
        name: release-name-kube-promethe-operator
        path: /admission-prometheusrules/validate
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-promethe-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission
    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name:  release-name-kube-promethe-admission
  namespace: monitor
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
imagePullSecrets:
  []
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:  release-name-kube-promethe-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - update
  - apiGroups: ['policy']
    resources: ['podsecuritypolicies']
    verbs:     ['use']
    resourceNames:
    - release-name-kube-promethe-admission
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:  release-name-kube-promethe-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-promethe-admission
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-admission
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  release-name-kube-promethe-admission
  namespace: monitor
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:  release-name-kube-promethe-admission
  namespace: monitor
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-kube-promethe-admission
subjects:
  - kind: ServiceAccount
    name: release-name-kube-promethe-admission
    namespace: monitor
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  release-name-kube-promethe-admission-create
  namespace: monitor
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission-create    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  template:
    metadata:
      name:  release-name-kube-promethe-admission-create
      labels:
        app: kube-prometheus-stack-admission-create        
        chart: kube-prometheus-stack-11.0.2
        release: "release-name"
        heritage: "Helm"
    spec:
      containers:
        - name: create
          image: jettech/kube-webhook-certgen:v1.5.0
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.monitor.svc
            - --namespace=monitor
            - --secret-name=release-name-kube-promethe-admission
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: release-name-kube-promethe-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: kube-prometheus-stack/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  release-name-kube-promethe-admission-patch
  namespace: monitor
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: kube-prometheus-stack-admission-patch    
    chart: kube-prometheus-stack-11.0.2
    release: "release-name"
    heritage: "Helm"
spec:
  template:
    metadata:
      name:  release-name-kube-promethe-admission-patch
      labels:
        app: kube-prometheus-stack-admission-patch        
        chart: kube-prometheus-stack-11.0.2
        release: "release-name"
        heritage: "Helm"
    spec:
      containers:
        - name: patch
          image: jettech/kube-webhook-certgen:v1.5.0
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=release-name-kube-promethe-admission
            - --namespace=monitor
            - --secret-name=release-name-kube-promethe-admission
            - --patch-failure-policy=Fail
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: release-name-kube-promethe-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
